@article{ponto2015,
  title = {Understanding and Evaluating Survey Research},
  author = {Julie Ponto},
  journal = {Journal of the advanced practitioner in oncology},
  year = {2015},
  volume = {6},
  number = {2},
  pages = {168-171},
}
@InCollection{check2012,
  booktitle = {Research methods in education},
  editor = {Joseph Check and Russell K. Schutt},
  title = {Survey Research},
  author = {Joseph Check and Russell K. Schutt},
  publisher = {Sage Publications},
  year = {2012},
  pages = {159-185},
  note = {10.4135/9781544307725},
  url = {https://methods.sagepub.com/book/research-methods-in-education},
}
@article{pfl1,
author = {Pfleeger, Shari Lawrence and Kitchenham, Barbara A.},
title = {Principles of Survey Research: Part 1: Turning Lemons into Lemonade},
year = {2001},
issue_date = {November 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/505532.505535},
doi = {10.1145/505532.505535},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {16–18},
numpages = {3},
} 
@article{kitch2,
author = {Kitchenham, Barbara A. and Pfleeger, Shari Lawrence},
title = {Principles of Survey Research Part 2: Designing a Survey},
year = {2002},
issue_date = {January 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/566493.566495},
doi = {10.1145/566493.566495},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {18–20},
numpages = {3},
}
@article{kitch3,
author = {Kitchenham, Barbara A. and Pfleeger, Shari Lawrence},
title = {Principles of Survey Research: Part 3: Constructing a Survey Instrument},
year = {2002},
issue_date = {March 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/511152.511155},
doi = {10.1145/511152.511155},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {20–24},
numpages = {5},
keywords = {question selection, questionnaire format, question types, survey construction, question construction},
}
@article{kitch4,
author = {Kitchenham, Barbara and Pfleeger, Shari Lawrence},
title = {Principles of Survey Research Part 4: Questionnaire Evaluation},
year = {2002},
issue_date = {May 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/638574.638580},
doi = {10.1145/638574.638580},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {20–23},
numpages = {4},
keywords = {researcher bias, survey validity, survey reliability, respondent motivation},
}
@article{kitch5,
author = {Kitchenham, Barbara and Pfleeger, Shari Lawrence},
title = {Principles of Survey Research: Part 5: Populations and Samples},
year = {2002},
issue_date = {September 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/571681.571686},
doi = {10.1145/571681.571686},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {17–20},
numpages = {4},
keywords = {survey methods, sampling, populations},
}
@article{kitch6,
author = {Kitchenham, Barbara and Pfleeger, Shari Lawrence},
title = {Principles of Survey Research Part 6: Data Analysis},
year = {2003},
issue_date = {March 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/638750.638758},
doi = {10.1145/638750.638758},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {24–27},
numpages = {4},
keywords = {statistical analysis, survey methods},
}
@manual{kasunic2005,
title = {Designing an effective survey},
author = {Mark Kasunic},
organization = {Software Engineering Institute},
year = {2005},
url = {https://resources.sei.cmu.edu/library/asset-view.cfm?assetid=7277},
}
@manual{glasow2005,
title = {Fundamentals of Survey Research Methodology},
author = {Priscilla A. Glasow},
organization = {Mitre Washington C3 Center},
year = {2005},
}
@inproceedings{moll2016,
author = {Moll\'{e}ri, Jefferson Seide and Petersen, Kai and Mendes, Emilia},
title = {Survey Guidelines in Software Engineering: An Annotated Review},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962619},
doi = {10.1145/2961111.2962619},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {58},
numpages = {6},
keywords = {Survey, annotated review, guidelines},
location = {Ciudad Real, Spain},
series = {ESEM ’16}
}
@InCollection{fowler2014,
  booktitle = {Survey Research Methods},
  editor = {Floyd Fowler},
  title = {Types of Errors in Surveys},
  edition = {4th},
  author = {Floyd Fowler},
  publisher = {Sage Publications},
  year = {2014},
  pages = {11-17},
  note = {10.4135/9781452230184},
  url = {https://methods.sagepub.com/book/survey-research-methods},
}
@Book{groves2009,
 author = {Groves, R. M., Fowler, F. J., Couper, M. P., Lepkowski, J. M., Singer, E. &amp Tourangeau, R. },
 title = {Survey methodology},
 publisher = {Wiley},
 year = {2009},
 address = {Hoboken, N.J},
 isbn = {978-0-470-46546-2},
 }
@inbook{beatty2019,
author = {Beatty, Paul and Cosenza, Carol and Fowler Jr., Floyd J.},
publisher = {John Wiley & Sons, Ltd},
isbn = {9781119083771},
title = {Experiments on the Design and Evaluation of Complex Survey Questions},
booktitle = {Experimental Methods in Survey Research},
chapter = {6},
pages = {113-129},
doi = {10.1002/9781119083771.ch6},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119083771.ch6},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119083771.ch6},
year = {2019},
keywords = {complex survey questions, conceptual complexity, dangling qualifiers, national surveys, questionnaire design decisions, randomized experiments, response categories},
abstract = {Summary Complex questions may appeal to survey researchers because they maximize the specificity of data while minimizing the number of distinct data items collected. This chapter provides examples of experiments designed to inform particular questionnaire design decisions in which the available guidance was limited or ambiguous. Each of the experiments begin with a dilemma involving draft questions proposed for national surveys, in which there was more than one plausible way to craft the question, and limited guidance regarding which was preferable. Initially, the dilemmas involved the optimal structure of questions that were long and grammatically complex, potentially including detailed descriptions and definitions, examples, explanations, multiple clauses, qualifiers, or including specific response categories. As the survey research progressed, it moved from matters of structural complexity to conceptual complexity – for example, considering the optimal way to manage questions containing disparate concepts, or ambiguity of question intent arising from context.}
}
@article{brenner2018,
author = {Philip S. Brenner and Justine Bulgar-Medina},
title ={Testing Mark-all-that-apply Measures of Sexual Orientation and Gender Identity},
journal = {Field Methods},
volume = {30},
number = {4},
pages = {357-370},
year = {2018},
doi = {10.1177/1525822X18795872},
URL = {https://doi.org/10.1177/1525822X18795872},
eprint = { https://doi.org/10.1177/1525822X18795872},
abstract = { Many social identities (e.g., race, ethnicity) are measured using mark-all-that-apply (MATA) questions because they allow survey respondents to account for the multiple, nonexclusive ways in which they identify themselves. We test the use of MATA measures of sexual orientation and gender identity and compare them with forced choice (FC), an alternative format using a series of yes-or-no questions. Respondents, including an oversample of lesbian, gay, bisexual, and queer (LGBQ) individuals, participated in a 2 × 3 factorial survey experiment. For the first factor, we hypothesize that respondents randomly assigned to FC will report a higher count of identities than those assigned to MATA. For the second factor, we hypothesize that increased topic salience will help LGBQ respondents in particular to overcome poor question design. Findings suggest that MATA and FC measure comparably when question writing best practices are followed, but topic salience can yield higher data quality when poorly formatted questions are used. }
}
@book{payne51,
author = {Payne, S.L.},
title = {The art of asking questions},
year = {1951},
publisher = {Princeton University Press},
}
@article{brenner2016,
author = {Philip S. Brenner and John DeLamater},
title ={Lies, Damned Lies, and Survey Self-Reports? Identity as a Cause of Measurement Bias},
journal = {Social Psychology Quarterly},
volume = {79},
number = {4},
pages = {333-354},
year = {2016},
doi = {10.1177/0190272516628298},

URL = { 
        https://doi.org/10.1177/0190272516628298
    
},
eprint = { 
        https://doi.org/10.1177/0190272516628298
    
}
,
    abstract = { Explanations of error in survey self-reports have focused on social desirability: that respondents answer questions about normative behavior to appear prosocial to interviewers. However, this paradigm fails to explain why bias occurs even in self-administered modes like mail and web surveys. We offer an alternative explanation rooted in identity theory that focuses on measurement directiveness as a cause of bias. After completing questions about physical exercise on a web survey, respondents completed a text message–based reporting procedure, sending updates on their major activities for five days. Random assignment was then made to one of two conditions: instructions mentioned the focus of the study, physical exercise, or not. Survey responses, text updates, and records from recreation facilities were compared. Direct measures generated bias—overreporting in survey measures and reactivity in the directive text condition—but the nondirective text condition generated unbiased measures. Findings are discussed in terms of identity. }
}
@article{fowler2016,
    author = {Fowler, Floyd J. and Roman, Anthony M. and Mahmood, Rumel and Cosenza, Carol A.},
    title = "{Reducing Nonresponse and Nonresponse Error in a Telephone Survey: An Informative Case Study}",
    journal = {Journal of Survey Statistics and Methodology},
    volume = {4},
    number = {2},
    pages = {246-262},
    year = {2016},
    month = {03},
    abstract = "{This paper presents the results of an effort to raise the response rate for a national survey of adults aged 40 years or older about a number of issues related to Medicare costs. The survey was initially planned as a list-assisted random digit dialing telephone survey, but when the early response rate experience was low, nonrespondents were sent a paper version of the survey by mail with a \\$10 incentive and asked to respond. The result was that about half of the telephone nonrespondents for whom we had a mail address returned a mail survey, and the 24 percent telephone response rate was raised to 58 percent overall. Moreover, of the sixteen key estimates on which the survey was focused, seven were significantly different as a result of the mail returns. Our experience demonstrates the potential for mail protocols to improve response rates on national surveys and provides another example of low response rates having important effects on the estimates made from survey data.}",
    issn = {2325-0984},
    doi = {10.1093/jssam/smw004},
    url = {https://doi.org/10.1093/jssam/smw004},
    eprint = {https://academic.oup.com/jssam/article-pdf/4/2/246/7956335/smw004.pdf},
}